"use strict";(self.webpackChunktoms_knowledge_hole=self.webpackChunktoms_knowledge_hole||[]).push([[4046],{3905:(e,t,a)=>{a.d(t,{Zo:()=>c,kt:()=>k});var o=a(7294);function n(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,o)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){n(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,o,n=function(e,t){if(null==e)return{};var a,o,n={},i=Object.keys(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||(n[a]=e[a]);return n}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)a=i[o],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(n[a]=e[a])}return n}var s=o.createContext({}),d=function(e){var t=o.useContext(s),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},c=function(e){var t=d(e.components);return o.createElement(s.Provider,{value:t},e.children)},u="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},p=o.forwardRef((function(e,t){var a=e.components,n=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),u=d(a),p=n,k=u["".concat(s,".").concat(p)]||u[p]||m[p]||i;return a?o.createElement(k,r(r({ref:t},c),{},{components:a})):o.createElement(k,r({ref:t},c))}));function k(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var i=a.length,r=new Array(i);r[0]=p;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l[u]="string"==typeof e?e:n,r[1]=l;for(var d=2;d<i;d++)r[d]=a[d];return o.createElement.apply(null,r)}return o.createElement.apply(null,a)}p.displayName="MDXCreateElement"},7172:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>s,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var o=a(7462),n=(a(7294),a(3905));const i={},r="Data Engineering JargonDump",l={unversionedId:"programming/data/data-engineering/data-engineering-jargon",id:"programming/data/data-engineering/data-engineering-jargon",title:"Data Engineering JargonDump",description:"Source//www.reddit.com/r/dataengineering/comments/rdw3b3/dataengineeringjargon/",source:"@site/docs/programming/data/data-engineering/data-engineering-jargon.md",sourceDirName:"programming/data/data-engineering",slug:"/programming/data/data-engineering/data-engineering-jargon",permalink:"/docs/programming/data/data-engineering/data-engineering-jargon",draft:!1,editUrl:"https://github.com/TeeWallz/teewallz_knowledge_hole/tree/main/docs/programming/data/data-engineering/data-engineering-jargon.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Data Engineering",permalink:"/docs/programming/data/data-engineering/"},next:{title:"Data Vault",permalink:"/docs/programming/data/data-vault"}},s={},d=[{value:"Data Dump",id:"data-dump",level:2},{value:"Data Pipelines",id:"data-pipelines",level:2},{value:"DBA",id:"dba",level:2},{value:"Data Warehouse",id:"data-warehouse",level:2},{value:"Data Mart",id:"data-mart",level:2},{value:"ODS",id:"ods",level:2},{value:"EDW",id:"edw",level:2},{value:"RDBMS",id:"rdbms",level:2},{value:"In-memory DB",id:"in-memory-db",level:2},{value:"Data Lake",id:"data-lake",level:2},{value:"Ingestion",id:"ingestion",level:2},{value:"Extract, Transform, Load (ETL)",id:"extract-transform-load-etl",level:2},{value:"Data Models",id:"data-models",level:2},{value:"Normalisation",id:"normalisation",level:2},{value:"Star schema",id:"star-schema",level:2},{value:"Facts",id:"facts",level:2},{value:"Dimensions",id:"dimensions",level:2},{value:"Schemas",id:"schemas",level:2},{value:"SCD (slowly changing dimension) Type 1\u20136",id:"scd-slowly-changing-dimension-type-16",level:2},{value:"Business Intelligence",id:"business-intelligence",level:2},{value:"Batch Processing",id:"batch-processing",level:2},{value:"T-SQL",id:"t-sql",level:2},{value:"NoSQL",id:"nosql",level:2},{value:"BTEQ",id:"bteq",level:2},{value:"Cloud",id:"cloud",level:2},{value:"Data Architecture",id:"data-architecture",level:2},{value:"Data Visualisation",id:"data-visualisation",level:2},{value:"Data Centres",id:"data-centres",level:2},{value:"Data Integration",id:"data-integration",level:2},{value:"Data Migration",id:"data-migration",level:2},{value:"Data Replication",id:"data-replication",level:2},{value:"Big Data",id:"big-data",level:2},{value:"Hive",id:"hive",level:2},{value:"HDFS",id:"hdfs",level:2},{value:"NiFi",id:"nifi",level:2},{value:"Kafka",id:"kafka",level:2},{value:"Flat File",id:"flat-file",level:2},{value:"Latency",id:"latency",level:2},{value:"Caching",id:"caching",level:2},{value:"Staging",id:"staging",level:2}],c={toc:d},u="wrapper";function m(e){let{components:t,...a}=e;return(0,n.kt)(u,(0,o.Z)({},c,a,{components:t,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"data-engineering-jargondump"},"Data Engineering JargonDump"),(0,n.kt)("p",null,"Source: ",(0,n.kt)("a",{parentName:"p",href:"https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/"},"https://www.reddit.com/r/dataengineering/comments/rdw3b3/data_engineering_jargon/")),(0,n.kt)("h2",{id:"data-dump"},"Data Dump"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A file or a table containing a significant amount of data to be analysed or transferred."),(0,n.kt)("p",{parentName:"blockquote"},'A table containing the "data dump" of all customer addresses.')),(0,n.kt)("h2",{id:"data-pipelines"},"Data Pipelines"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A data processing method akin to a pipeline, which starts with data ingestion then processing then completion."),(0,n.kt)("p",{parentName:"blockquote"},"A pipeline where customer address data is ingested from source A and then aggregated according to their cities and this new information is loaded into destination B.")),(0,n.kt)("h2",{id:"dba"},"DBA"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Database Administrator is an admin role that understands the particular database technology and how to get the best out of it. This includes improving performance, backups and recovery."),(0,n.kt)("p",{parentName:"blockquote"},"Performance tuning the database to respond better to particular complex data queries.")),(0,n.kt)("h2",{id:"data-warehouse"},"Data Warehouse"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A method of organising data to make it easy to analyse and report to make business decisions"),(0,n.kt)("p",{parentName:"blockquote"},"Oracle data warehouse. Organising customer data in a data warehouse to be able to report the number of newly acquired customers.")),(0,n.kt)("h2",{id:"data-mart"},"Data Mart"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A subset of a data warehouse, created for a very specific business use case."),(0,n.kt)("p",{parentName:"blockquote"},"Finance data mart storing all the relevant financial information required by the Accounting team to process their month-end cycles.")),(0,n.kt)("h2",{id:"ods"},"ODS"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Operational data store generally stores limited and current information to help simple queries. Unable to handle historical or complex data queries."),(0,n.kt)("p",{parentName:"blockquote"},"An ODS for daily stock fluctuations in a warehouse help the warehouse manager decide what to prioritise in the next order delivery.")),(0,n.kt)("h2",{id:"edw"},"EDW"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The same as a data warehouse except it includes all the data within an organisation. This means that the entire enterprise can rely on this warehouse for their business decisions."),(0,n.kt)("p",{parentName:"blockquote"},"Organising sales, customer, marketing and finance data in an enterprise data warehouse to be able to create several key management reports.")),(0,n.kt)("h2",{id:"rdbms"},"RDBMS"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Relational database management system. All of the above examples are RDBMS, meaning they store data in a structured format using rows and columns."),(0,n.kt)("p",{parentName:"blockquote"},"A Microsoft SQL server database.")),(0,n.kt)("h2",{id:"in-memory-db"},"In-memory DB"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Traditional databases have been used for complex calculations and queries. They store information on the actual disk in the computer. In-memory DB stores all the information on their memory (RAM), this allows for rapid calculations without read and write a function to a normal disk."),(0,n.kt)("p",{parentName:"blockquote"},"A drill-down functionality of a live dashboard.")),(0,n.kt)("h2",{id:"data-lake"},"Data Lake"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A repository for all kinds of structured and unstructured data. Mainly based on Hadoop storage technology. Called a lake as it is flexible enough to store anything from raw data to unstructured email files."),(0,n.kt)("p",{parentName:"blockquote"},"Hadoop Data Lake. Storing logs of all customers called into the inbound call centre including call duration.")),(0,n.kt)("h2",{id:"ingestion"},"Ingestion"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Generally, the first step in a data pipeline, where data is inserted in tables in the platform."),(0,n.kt)("p",{parentName:"blockquote"},"A pipeline where customer address data is inserted from source A.")),(0,n.kt)("h2",{id:"extract-transform-load-etl"},"Extract, Transform, Load (ETL)"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A 3-step process of extracting data and transforming it (by applying some kind of logic like aggregation) and loading the new information into the destination. It could be used as ELT where the destination tables transform the data instead."),(0,n.kt)("p",{parentName:"blockquote"},"An extract of customer address data is taken from the customer relationship management tool and is then aggregated according to their cities and this new information is loaded into destination B.")),(0,n.kt)("h2",{id:"data-models"},"Data Models"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A way of organising the data in a way that it can be understood in a real-world scenario."),(0,n.kt)("p",{parentName:"blockquote"},"Taking a huge amount of data and logically grouping it into customer, product and location data.")),(0,n.kt)("h2",{id:"normalisation"},"Normalisation"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A method of organising the data in a granular enough format that it can be utilised for different purposes over time. Usually, this is done by normalising the data into different forms such as 1NF (normal form) or 3NF (3rd normal form) which is the most common."),(0,n.kt)("p",{parentName:"blockquote"},"Taking customer order data and creating granular information model; order in one table, item ordered in another table, customer contact in another table, payment of the order in another table. This allows for the data to be re-used for different purposes over time.")),(0,n.kt)("h2",{id:"star-schema"},"Star schema"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The simplest way to model data into different quantitative and qualitative data is called facts and dimensions. Usually, the fact table is interpreted with the help of a dimensions table resembling a star."),(0,n.kt)("p",{parentName:"blockquote"},"A Star schema of sales data with dimensions such as customer, product & time.")),(0,n.kt)("h2",{id:"facts"},"Facts"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A data warehousing term for quantitative information."),(0,n.kt)("p",{parentName:"blockquote"},"The number of orders placed by a customer.")),(0,n.kt)("h2",{id:"dimensions"},"Dimensions"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A data warehousing term for qualitative information."),(0,n.kt)("p",{parentName:"blockquote"},"Name of the customer or their country of residence.")),(0,n.kt)("h2",{id:"schemas"},"Schemas"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A term for a collection of database objects. These are generally used to logically separate data within the database and apply access controls."),(0,n.kt)("p",{parentName:"blockquote"},"Storing HR data in HR schema allows logical segregation from other data in the organisation.")),(0,n.kt)("h2",{id:"scd-slowly-changing-dimension-type-16"},"SCD (slowly changing dimension) Type 1\u20136"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A method to deal with changes in the data over time in a data warehouse. Type 1 is when history is overwritten whereas Type 2 (most common) is when history is maintained each time a change occurs."),(0,n.kt)("p",{parentName:"blockquote"},"When a customer changes their address; SCD Type 1 would overwrite the old address with the new one, whereas Type 2 would store both addresses to maintain history.")),(0,n.kt)("h2",{id:"business-intelligence"},"Business Intelligence"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A slightly out of date term for a combination of practices to derive business insights from data by predominantly using data warehousing, analytics and dashboarding."),(0,n.kt)("p",{parentName:"blockquote"},"Creating a management dashboard to show customer demographics across the country.")),(0,n.kt)("h2",{id:"batch-processing"},"Batch Processing"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"An automated way of processing millions of data transactions at the same time. This is  generally carried out overnight with the help of \u201cbatch jobs\u201d."),(0,n.kt)("p",{parentName:"blockquote"},"Loading all the customer\u2019s data that bought a particular item on the day.")),(0,n.kt)("h2",{id:"t-sql"},"T-SQL"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"SQL is a Structured Query Language or, simply put, a language used to manage databases. T-SQL is Transact-SQL which is a proprietary Microsoft extension of the SQL language."),(0,n.kt)("p",{parentName:"blockquote"},"T-SQL can be used MS SQL Server or Azure SQL Database to write a statement as follows \u201cSELECT customer_name from tbl_customer_information where customer_city = \u201cLondon\u201d. This provides the result of all the customer names where customers are based in London.")),(0,n.kt)("h2",{id:"nosql"},"NoSQL"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Although SQL has been around for decades, NoSQL (not only SQL) is a concept designed for non-relational databases, particularly to store unstructured data like documents."),(0,n.kt)("p",{parentName:"blockquote"},"Storing an Outlook email file in XML with key-value pair on a MongoDB document database.")),(0,n.kt)("h2",{id:"bteq"},"BTEQ"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Batch Teradata Query (like SQL) is simply a utility and query tool for Teradata, which is a relational database system"),(0,n.kt)("p",{parentName:"blockquote"},"Creating a BTEQ script to load data from a flat file.")),(0,n.kt)("h2",{id:"cloud"},"Cloud"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Delivery of computing services such as servers, networking, analytics etc., over the internet instead of using a dedicated data centre for an organisation."),(0,n.kt)("p",{parentName:"blockquote"},"Storing data on Microsoft\u2019s Azure Cloud service instead of on an on-premise solution.")),(0,n.kt)("h2",{id:"data-architecture"},"Data Architecture"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The discipline of managing the people, processes and technologies relating to data; includes data strategy, data capture processes and technical patterns to derive insight from the data."),(0,n.kt)("p",{parentName:"blockquote"},"A Data Architect creates a framework for an enterprise to manage its data flow end to end.")),(0,n.kt)("h2",{id:"data-visualisation"},"Data Visualisation"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A practice for visualising large amounts of data to derive key insights to drive business decisions."),(0,n.kt)("p",{parentName:"blockquote"},"An executive dashboard that clearly outlines the sales performance of a certain team.")),(0,n.kt)("h2",{id:"data-centres"},"Data Centres"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A dedicated space (nowadays millions of sqft of space) which houses servers and systems for the organisation\u2019s critical applications"),(0,n.kt)("p",{parentName:"blockquote"},"Microsoft Data Centre to host all the company\u2019s critical applications.")),(0,n.kt)("h2",{id:"data-integration"},"Data Integration"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Usually, the hardest part of the project, where multiple sources of data are integrated into a singular application/data warehouse."),(0,n.kt)("p",{parentName:"blockquote"},"Integrating finance and customer relationship systems integrating into an MS SQL server database.")),(0,n.kt)("h2",{id:"data-migration"},"Data Migration"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The practice of migrating the data from source to destination"),(0,n.kt)("p",{parentName:"blockquote"},"Migrating data from MS SQL server database to an Amazon Relational Database service")),(0,n.kt)("h2",{id:"data-replication"},"Data Replication"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"There are multiple ways to do this, but mainly it is a practice of copying data to multiple servers to protect an organisation against data loss."),(0,n.kt)("p",{parentName:"blockquote"},"Replicating the customer information across two databases to ensure their core details are not lost.")),(0,n.kt)("h2",{id:"big-data"},"Big Data"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"A term coined for large amounts of data that cannot be processed using traditional databases. Refer to Data Lake."),(0,n.kt)("p",{parentName:"blockquote"},"EDIT: The 3 Vs that are defining properties of Big Data are Volume, Velocity & Variety."),(0,n.kt)("p",{parentName:"blockquote"},"u/mrchowmein"),(0,n.kt)("ul",{parentName:"blockquote"},(0,n.kt)("li",{parentName:"ul"},"thanks for the tip!")),(0,n.kt)("p",{parentName:"blockquote"},"Hadoop Data Lake stores all the information received from sensors in a smart fridge.")),(0,n.kt)("h2",{id:"hive"},"Hive"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Apache Hive is a data warehouse open-source project that allows querying large amounts of data. Like SQL, it uses an easy-to-understand language called Hive QL."),(0,n.kt)("p",{parentName:"blockquote"},"EDIT: Hive is the project that brought SQL querying capacities to Hadoop. An important point is that it's not a full database, because it doesn't manage the storage layer, but rather the combination of a query engine (turning SQL into map-reduce jobs) and a meta-store (table schemas, locations, statistics...) that allows querying existing data storage tools such as HDFS or other big data databases like HBase and many others."),(0,n.kt)("p",{parentName:"blockquote"},"u/sib_n"),(0,n.kt)("ul",{parentName:"blockquote"},(0,n.kt)("li",{parentName:"ul"},"thanks for additional information")),(0,n.kt)("p",{parentName:"blockquote"},"SELECT \\ from tbl; returns all rows and columns from a data store like HDFS.*")),(0,n.kt)("h2",{id:"hdfs"},"HDFS"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Hadoop Distributed File System is a data storage system used by Hadoop. It provides flexibility to manage structured or unstructured data."),(0,n.kt)("p",{parentName:"blockquote"},"Storing large amounts of financial transactional data in an HDFS to query using Hive QL.")),(0,n.kt)("h2",{id:"nifi"},"NiFi"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"It is an open-source extract, transform and load tool (refer to ETL); this allows filtering, integrating and joining data."),(0,n.kt)("p",{parentName:"blockquote"},"Moving postcode data from a .csv file to HDFS using NiFi.")),(0,n.kt)("h2",{id:"kafka"},"Kafka"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"It is more complex to work with than NiFi as it doesn\u2019t have a user interface (UI), mainly used for real-time streaming data. It is a messaging system first created by LinkedIn engineers."),(0,n.kt)("p",{parentName:"blockquote"},"Streaming real-time weather events using Kafka")),(0,n.kt)("h2",{id:"flat-file"},"Flat File"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"Flat files are commonly used to transfer data due to their basic nature; flat files are a single table storing data in a plain text format."),(0,n.kt)("p",{parentName:"blockquote"},"EDIT: no nested fields which are common in other types of files such as XML and JSON (flat JSON-lines files are very common though). Nested structures are generally more disk-space efficient, but they are more complicated to parse and load elsewhere than flat files."),(0,n.kt)("p",{parentName:"blockquote"},"u/sib_n"),(0,n.kt)("ul",{parentName:"blockquote"},(0,n.kt)("li",{parentName:"ul"},"thanks for additional information")),(0,n.kt)("p",{parentName:"blockquote"},"All customer order numbers stored in a comma-separated value (.csv) file")),(0,n.kt)("h2",{id:"latency"},"Latency"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The time it takes for a database or a web application to respond to a query or a click."),(0,n.kt)("p",{parentName:"blockquote"},"Takes 30 seconds to query a database with 5 million records.")),(0,n.kt)("h2",{id:"caching"},"Caching"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"This is when limited data is stored on the RAM to allow for quick retrieval of information."),(0,n.kt)("p",{parentName:"blockquote"},"In-memory caching of data in a database returns results to query 100 times faster.")),(0,n.kt)("h2",{id:"staging"},"Staging"),(0,n.kt)("blockquote",null,(0,n.kt)("p",{parentName:"blockquote"},"The name of a storage area that is temporary in nature; to allow for processing of ETL jobs (refer to ETL)."),(0,n.kt)("p",{parentName:"blockquote"},'EDIT: Depending on the design of the ETL, this could be temporary or persistent but it is always an "intermediate" step rather than the final output table.'),(0,n.kt)("p",{parentName:"blockquote"},"A staging area in an ETL routine allows data to be cleaned before loading into the final tables.")))}m.isMDXComponent=!0}}]);